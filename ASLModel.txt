# ======================
# 1. Mount Google Drive
# ======================
from google.colab import drive
import os

# 1. Mount Google Drive first
drive.mount('/content/drive')

# Create and go to project folder in Google Drive
PROJECT_PATH = "/content/drive/MyDrive/ASL_Project"
os.makedirs(PROJECT_PATH, exist_ok=True)
%cd $PROJECT_PATH

print(f"‚úÖ Project folder ready at: {PROJECT_PATH}")


# ======================
# 2. Install Dependencies (Latest Versions)
# ======================
!pip install --upgrade pip
!pip install tensorflow opencv-python mediapipe scikit-learn matplotlib

# ======================
# 3. Import Libraries
# ======================
import cv2
import mediapipe as mp
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense
from tensorflow.keras.preprocessing.sequence import pad_sequences

print("TensorFlow Version:", tf.__version__)

# ======================
# 4. Initialize MediaPipe for BOTH hands
# ======================
mp_hands = mp.solutions.hands
hands = mp_hands.Hands(
    static_image_mode=False,
    max_num_hands=2,
    min_detection_confidence=0.5,
    min_tracking_confidence=0.5
)
mp_drawing = mp.solutions.drawing_utils

# ======================
# 5. Function to Extract Landmarks from Video (BOTH HANDS)
# ======================
def extract_landmarks_from_video(video_path):
    cap = cv2.VideoCapture(video_path)
    landmarks_sequence = []

    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break

        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        results = hands.process(image)

        left_hand = [0] * 63
        right_hand = [0] * 63

        if results.multi_hand_landmarks and results.multi_handedness:
            for hand_landmarks, handedness in zip(results.multi_hand_landmarks, results.multi_handedness):
                landmarks = []
                for lm in hand_landmarks.landmark:
                    landmarks.extend([lm.x, lm.y, lm.z])

                if handedness.classification[0].label == "Left":
                    left_hand = landmarks
                else:
                    right_hand = landmarks

        frame_landmarks = left_hand + right_hand  # 126 values
        landmarks_sequence.append(frame_landmarks)

    cap.release()
    return np.array(landmarks_sequence)

# ======================
# 6. Prepare Dataset
# ======================
DATA_PATH = os.path.join(PROJECT_PATH, "dataset")
if not os.path.exists(DATA_PATH):
    os.makedirs(DATA_PATH)
    print(f"‚ö†Ô∏è Dataset folder created at {DATA_PATH}. Please add your videos in label folders.")
else:
    print(f"üìÇ Using dataset from {DATA_PATH}")

"""```
# Folder structure should be:
# dataset/
#   hello/
#     video1.mp4
#     video2.mp4
#   thank_you/
#     video1.mp4
#     video2.mp4
```
"""

actions = os.listdir(DATA_PATH)
if not actions:
    print("‚ùå No labels found in dataset folder. Please upload videos first.")

X, y = [], []
for label in actions:
    folder_path = os.path.join(DATA_PATH, label)
    if not os.path.isdir(folder_path):
        continue
    for file in os.listdir(folder_path):
        video_path = os.path.join(folder_path, file)
        print(f"üîç Processing {video_path} ...")
        sequence = extract_landmarks_from_video(video_path)
        X.append(sequence)
        y.append(label)

# Pad sequences to equal length
X = pad_sequences(X, padding='post', dtype='float32')

# Encode labels
le = LabelEncoder()
y = le.fit_transform(y)

# Train/test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# ======================
# 7. Build LSTM Model
# ======================
from tensorflow.keras import Input

model = Sequential()
model.add(Input(shape=(X_train.shape[1], X_train.shape[2])))
model.add(LSTM(64, return_sequences=True, activation='relu'))
model.add(LSTM(128, return_sequences=False, activation='relu'))
model.add(Dense(64, activation='relu'))
model.add(Dense(len(actions), activation='softmax'))

model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

model.summary()

# ======================
# 8. Train Model & Export
# ======================
history = model.fit(X_train, y_train, epochs=20, validation_data=(X_test, y_test))

# Save standard Keras model

model_h5_path = os.path.join(PROJECT_PATH, "asl_lstm_model.h5")
model_tflite_path = os.path.join(PROJECT_PATH, "asl_lstm_model.tflite")

model.save(model_h5_path)

converter = tf.lite.TFLiteConverter.from_keras_model(model)

# Allow TensorFlow ops that TFLite doesn‚Äôt fully support
converter.target_spec.supported_ops = [
    tf.lite.OpsSet.TFLITE_BUILTINS,  # standard TFLite ops
    tf.lite.OpsSet.SELECT_TF_OPS     # include TF ops fallback
]

# Prevent lowering of tensor list ops (needed for LSTM)
converter._experimental_lower_tensor_list_ops = False

# Optional: enable optimizations for smaller model size
converter.optimizations = [tf.lite.Optimize.DEFAULT]

tflite_model = converter.convert()

with open(model_tflite_path, "wb") as f:
    f.write(tflite_model)

print(f"‚úÖ TFLite model saved at {model_tflite_path}")

# ======================
# 9. Plot Accuracy/Loss
# ======================

plt.figure(figsize=(10, 5))
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.legend()
plt.show()
